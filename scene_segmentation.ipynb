{
 "cells": [
  {
   "cell_type": "raw",
   "id": "6a715b27-78ae-4bee-9e67-aad0e5b2efb7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AnH1oLv8IzJV",
    "outputId": "d2a24b51-997f-466b-eb4b-a712608b1bb6",
    "scrolled": true,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "!unzip highway.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ade700e-1f9a-4268-a809-4f829ce411c8",
   "metadata": {
    "id": "5ade700e-1f9a-4268-a809-4f829ce411c8"
   },
   "source": [
    "# Highway Scene Segmentation\n",
    "\n",
    "This notebook showcases semantic scene segmentation into background pixels and a foreground with driving cars. Both training and validation data are taken from the highway scene in the Change Detection dataset, which consists of labeled highway camera frames (except the first 470 which are unlabeled): http://jacarini.dinf.usherbrooke.ca/dataset2014#\n",
    "\n",
    "Input: \n",
    "  - RGB \n",
    "  - Shape (3,320,240)\n",
    "\n",
    "Label:\n",
    "  - Greyscale\n",
    "  - 0:black:background, 170:grey:foreground-edges,  255:white:foreground\n",
    "  - Shape (1,320,240)\n",
    "\n",
    "\n",
    "![input image](showcase/in001600.jpg \"Title\") ![gt image](showcase/gt001600.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510bfc43-f5cc-4a72-bc1b-e204d3c2c987",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc62465-2687-415e-a780-877463a91c03",
   "metadata": {
    "id": "5cc62465-2687-415e-a780-877463a91c03"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, jaccard_score\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b5923-78bb-48e7-8db6-79b35b79b942",
   "metadata": {
    "id": "ea8b5923-78bb-48e7-8db6-79b35b79b942"
   },
   "source": [
    "## Pytorch Dataset Wrapper\n",
    "Inherits from **torch.utils.data.Dataset** and implements two methods.\n",
    "- **def \\_\\_getitem\\_\\_(self, idx)**: given an integer idx returns the data x,y\n",
    "    - x is the image as a float tensor of shape: $(3,H,W)$\n",
    "    - y is the label image as a mask of shape: $(H,W)$ each pixel should contain the label 0 (background) or 1 (foreground). It is recommended to use the type torch.long\n",
    "\n",
    "Image resolution is decreased to fit at once into memory and GPU. Some regularization through color jittering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b6c3f9-7bb1-400b-8f8a-5d87a9078d64",
   "metadata": {
    "id": "e0b6c3f9-7bb1-400b-8f8a-5d87a9078d64"
   },
   "outputs": [],
   "source": [
    "augmentations = v2.Compose([\n",
    "    v2.ColorJitter(brightness=0.5, contrast=1.5, saturation=2.5, hue=0.5),\n",
    "    v2.RandomResizedCrop(size=(224, 224),scale=(0.7, 1.0)),\n",
    "    v2.RandomRotation(30),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "    #v2.ToDtype(torch.float32, scale=True),\n",
    "    #v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1706343b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input = read_image(\"showcase/in001600.jpg\").float()/255\n",
    "label = read_image(\"showcase/gt001600.png\")\n",
    "\n",
    "augmented_img, augmented_mask = augmentations(\n",
    "    read_image(\"showcase/in001600.jpg\").float()/255,\n",
    "    tv_tensors.Mask(label==255)\n",
    "    )\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "\n",
    "plt.subplot(1, 7, 1)\n",
    "plt.imshow(input.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"input\")\n",
    "\n",
    "plt.subplot(1, 7, 2)\n",
    "plt.imshow((input*(np.repeat(label, 3, axis=0)==0)).permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"background\")\n",
    "\n",
    "plt.subplot(1, 7, 3)\n",
    "plt.imshow((input*(np.repeat(label, 3, axis=0)==170)).permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"shadow\")\n",
    "\n",
    "plt.subplot(1, 7, 4)\n",
    "plt.imshow((input*(np.repeat(label, 3, axis=0)==255)).permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"cars\")\n",
    "\n",
    "plt.subplot(1, 7, 5)\n",
    "plt.imshow(label.squeeze(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"label\")\n",
    "\n",
    "plt.subplot(1, 7, 6)\n",
    "plt.imshow(augmented_img.permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"aug_input\")\n",
    "\n",
    "plt.subplot(1, 7, 7)\n",
    "plt.imshow(augmented_mask.squeeze(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"aug_label\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a300af64",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HighwayDataset(Dataset):\n",
    "    def __init__(self, device, augmentation=None):\n",
    "        self.input_filenames = sorted(os.listdir(\"highway/input\"))[470:]\n",
    "        self.labels_filenames = sorted(os.listdir(\"highway/groundtruth\"))[470:]\n",
    "        self.augmentation = augmentation\n",
    "        self.device = device\n",
    "\n",
    "        self.inputs = []\n",
    "        self.labels = []\n",
    "\n",
    "        for i in range(len(self.input_filenames)):\n",
    "            image = read_image( \"highway/input/\" + self.input_filenames[i]).float()/255\n",
    "            label = (read_image( \"highway/groundtruth/\" + self.labels_filenames[i])[0] > 0).long()\n",
    "\n",
    "            self.inputs.append(image)\n",
    "            self.labels.append(label)\n",
    "\n",
    "        self.inputs = torch.stack(self.inputs).to(device)\n",
    "        self.labels = torch.stack(self.labels).to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        augmented_img, augmented_mask = augmentations(\n",
    "            self.inputs[idx],\n",
    "            tv_tensors.Mask(self.labels[idx])\n",
    "            )\n",
    "        return augmented_img, augmented_mask\n",
    "    \n",
    "highwayDataset = HighwayDataset(device)\n",
    "highwayDataset.inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74252cd6-d8a5-4e01-aee2-92accc20a6d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "74252cd6-d8a5-4e01-aee2-92accc20a6d2",
    "outputId": "91f8decc-5f07-4f1b-91e4-b2146d9240f3"
   },
   "outputs": [],
   "source": [
    "highwayDataset = HighwayDataset(device)\n",
    "highwayDataset.inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e120eb7-0349-4bb0-b305-23a0f3bb5e26",
   "metadata": {
    "id": "0e120eb7-0349-4bb0-b305-23a0f3bb5e26"
   },
   "source": [
    "## Fully-Convolutional Neural Network\n",
    "The CNN inspired by U-Net is flexible to the input and output resolution and includes residual blocks.\n",
    "\n",
    "- input: a batch of images $(B,3,H,W)$\n",
    "- output: a batch of pixel-wise class predictions $(B,C,H,W)$, where $C=2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0f819d-863c-4256-841a-3d1062f8c700",
   "metadata": {
    "id": "cd0f819d-863c-4256-841a-3d1062f8c700"
   },
   "outputs": [],
   "source": [
    "class SmallUNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallUNet, self).__init__()\n",
    "\n",
    "        self.sideconv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.sideconv2 = nn.Conv2d(64, 128, 3, 1, 1)\n",
    "        self.sideconv3 = nn.Conv2d(128, 256, 3, 1, 1)\n",
    "        self.sideconv4 = nn.Conv2d(256, 256, 3, 1, 1)\n",
    "        self.upconv1 = nn.ConvTranspose2d(256, 256, 2, 2)\n",
    "        self.sideconv5 = nn.Conv2d(512, 128, 3, 1, 1)\n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 128, 2, 2)\n",
    "        self.sideconv6 = nn.Conv2d(256, 64, 3, 1, 1)\n",
    "        self.upconv3 = nn.ConvTranspose2d(64, 64, 2, 2)\n",
    "        self.sideconv7 = nn.Conv2d(128, 1, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Encoder\n",
    "        x = self.sideconv1(x)\n",
    "        x_res1 = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.sideconv2(x)\n",
    "        x_res2 = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        x = self.sideconv3(x)\n",
    "        x_res3 = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Bottleneck\n",
    "        x = self.sideconv4(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Decoder\n",
    "        x = self.upconv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.cat((x, x_res3), dim=1)\n",
    "        x = self.sideconv5(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.upconv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.cat((x, x_res2), dim=1)\n",
    "        x = self.sideconv6(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.upconv3(x)\n",
    "        x = F.relu(x)\n",
    "        x = torch.cat((x, x_res1), dim=1)\n",
    "        output = self.sideconv7(x)\n",
    "\n",
    "        return F.sigmoid(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85364b0-7010-4710-9373-94a89db8d7c5",
   "metadata": {
    "id": "a85364b0-7010-4710-9373-94a89db8d7c5"
   },
   "source": [
    "## Train and Test(Validation) Function\n",
    "- Loss: Cross Entropy\n",
    "- Metrics:\n",
    "    - Pixel-wise: Accuracy, Precision, Recall\n",
    "    - Image-wise: Intersection over Union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25db36f6-83aa-4875-9f9b-6295e9074442",
   "metadata": {
    "id": "25db36f6-83aa-4875-9f9b-6295e9074442"
   },
   "outputs": [],
   "source": [
    "def train_classifier(model, device, train_loader, optimizer, epoch, loss_list):\n",
    "    model.train()\n",
    "\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.cross_entropy(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        #if batch_idx%5 == 0:\n",
    "        #    print(\"train batch id:\", batch_idx,\" data len:\",len(data), \" train batch loss:\", loss.item())\n",
    "\n",
    "        print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, (batch_idx+1) * len(data), len(train_loader.dataset),\n",
    "            100. * (batch_idx+1) / len(train_loader), loss.item()), end='\\r')\n",
    "\n",
    "\n",
    "    loss_list.append(epoch_loss / len(train_loader))\n",
    "\n",
    "\n",
    "def test_classifier(model, device, test_loader, loss_list):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            last_loss = F.cross_entropy(output, target).item()#, reduction='sum').item()\n",
    "            #if batch_idx%2 == 0:\n",
    "            #    print(\"test batch id:\", batch_idx,\" data len:\",len(data), \" test batch loss:\", last_loss)\n",
    "\n",
    "            test_loss += last_loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "\n",
    "\n",
    "    test_loss /= len(test_loader)#.dataset * 60 * 80 )\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset) *60*80 ,\n",
    "    100. * correct / (len(test_loader.dataset)* 60 * 80 ), end='\\r')) # / 60 * 80  wegen number of images * height *width  pixel insgesamt\n",
    "\n",
    "    loss_list.append(test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3930b15-5a24-464f-b00d-d3d469775291",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "- 80/20 Data Split\n",
    "- PyTorch DataLoaders\n",
    "- Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7a24a0-1c68-498c-9858-252402d1861a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4b7a24a0-1c68-498c-9858-252402d1861a",
    "outputId": "6b5461d1-21ce-4b0f-d723-03420826b75e"
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = random_split(highwayDataset, [0.8, 0.2])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=512)\n",
    "\n",
    "model = SmallUNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.002)#vorher lr 0.001\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)# vorher step_size 5\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(1,epochs + 1):\n",
    "    train_classifier(model, device, train_loader, optimizer, epoch, train_losses)\n",
    "    test_classifier(model, device, test_loader, test_losses)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c719a951-5dc6-4a9d-b882-bf4fc0859555",
   "metadata": {
    "id": "c719a951-5dc6-4a9d-b882-bf4fc0859555"
   },
   "outputs": [],
   "source": [
    "def get_segmentation(model, device, test_loader):\n",
    "    model.eval()\n",
    "    segmentations = []\n",
    "    accuracies = []\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    ious = []\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(test_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "\n",
    "            segmentations.append(pred.detach().cpu().numpy())\n",
    "\n",
    "            for i in range(data.size(0)):  \n",
    "                pred_flat = pred[i].cpu().numpy().flatten()\n",
    "                target_flat = target[i].cpu().numpy().flatten()\n",
    "\n",
    "                accuracies.append(accuracy_score(target_flat, pred_flat))\n",
    "                precisions.append(precision_score(target_flat, pred_flat, average='macro'))\n",
    "                recalls.append(recall_score(target_flat, pred_flat, average='macro'))\n",
    "                ious.append(jaccard_score(target_flat, pred_flat, average='macro'))\n",
    "\n",
    "    #print(accuracies, precisions, recalls, ious)\n",
    "    return np.concatenate(segmentations), np.array(accuracies), np.array(precisions), np.array(recalls), np.array(ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1109e5d-a62a-4210-b068-2a8dbff711bd",
   "metadata": {
    "id": "a1109e5d-a62a-4210-b068-2a8dbff711bd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results, acc, prec, rec, iou = get_segmentation(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d6877-608d-44c5-aead-f96ab16aba50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7b5d6877-608d-44c5-aead-f96ab16aba50",
    "outputId": "1c8f3376-e20f-405b-aa86-4720810d286a"
   },
   "outputs": [],
   "source": [
    "results.shape, acc.shape, prec.shape, rec.shape, iou.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e603ff-a32e-4d90-84d9-eba8570d6e63",
   "metadata": {
    "id": "83e603ff-a32e-4d90-84d9-eba8570d6e63"
   },
   "source": [
    "## Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02cc989-9709-4d6b-af4c-0c28692c2e50",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "b02cc989-9709-4d6b-af4c-0c28692c2e50",
    "outputId": "75c6f0d0-54ac-463a-a446-5c51a255d94a"
   },
   "outputs": [],
   "source": [
    "example_output = model(highwayDataset[10030][0]).detach()\n",
    "example_probabilities = F.softmax(example_output, dim=0)\n",
    "example_segmentation = example_probabilities.argmax(dim=0)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.subplot(1, 5, 1)\n",
    "plt.imshow(highwayDataset[10030][0].detach().cpu().permute(1, 2, 0))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Image\")\n",
    "\n",
    "plt.subplot(1, 5, 2)\n",
    "plt.imshow(example_output[0].detach().cpu(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Back Probs\")\n",
    "\n",
    "plt.subplot(1, 5, 3)\n",
    "#plt.imshow(torch.squeeze(model(d[10030][0]).detach(),0))\n",
    "plt.imshow(example_output[1].detach().cpu(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Front Probs\")\n",
    "\n",
    "plt.subplot(1, 5, 4)\n",
    "#plt.imshow(torch.squeeze(model(d[10030][0]).detach(),0))\n",
    "plt.imshow(example_segmentation.detach().cpu(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Back Pred\")\n",
    "\n",
    "plt.subplot(1, 5, 5)\n",
    "plt.imshow(highwayDataset[10030][1].detach().cpu(), cmap=\"gray\")\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"GT\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a0a734-3078-4da2-ad16-f04fb51bfee3",
   "metadata": {},
   "source": [
    "# Training and Test Error over each Epoch\n",
    "Later comment: TODO Looks very suspicious. Must be revisited. EDIT: Data split and loader setup seem correct. Did not find any leakage yet. Maybe investigate training vs val/test loss without training data augmentation? still suspicious.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a13b5f-828e-47f3-bd66-f1a20d1bee02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "74a13b5f-828e-47f3-bd66-f1a20d1bee02",
    "outputId": "8d6f6d6a-a209-4332-bc51-9b800279c7d0"
   },
   "outputs": [],
   "source": [
    "x = [ i+1 for i in range(epochs)]\n",
    "\n",
    "\n",
    "plt.plot(x, train_losses, label='training loss')\n",
    "plt.plot(x, test_losses, label='test loss')\n",
    "\n",
    "plt.title('training and test loss over epochs')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f9b58f-23ec-4df3-8c35-e777c61486fd",
   "metadata": {},
   "source": [
    "## Evaluation Metrics for Final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a5276e-06bf-4eef-8033-d0fdf6a56de9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "95a5276e-06bf-4eef-8033-d0fdf6a56de9",
    "outputId": "71cdfaa3-e735-4774-a783-d68eeb644056"
   },
   "outputs": [],
   "source": [
    "print(highwayDataset.gts[test_dataset.indices].shape)\n",
    "print(classification_report(highwayDataset.gts[test_dataset.indices].detach().cpu().flatten(), results.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e171425d-4ad6-48b4-8193-6bbde22f801b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "e171425d-4ad6-48b4-8193-6bbde22f801b",
    "outputId": "7428edf6-5e09-4bf1-c2ac-5ff9c3729b1a"
   },
   "outputs": [],
   "source": [
    "print(acc)\n",
    "plt.hist(acc)\n",
    "plt.title(\"Accuracy for each image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81dd83c-2520-4aa3-821c-28b1789b322a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "f81dd83c-2520-4aa3-821c-28b1789b322a",
    "outputId": "63b65996-7116-4642-e770-2f1dcc21e76a"
   },
   "outputs": [],
   "source": [
    "print(prec)\n",
    "plt.hist(prec)\n",
    "plt.title(\"Precision for each image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a7f98f-1ffd-4a45-9729-cb5661c05328",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "53a7f98f-1ffd-4a45-9729-cb5661c05328",
    "outputId": "baf55ece-fd81-49ef-aee7-9feef187d93f"
   },
   "outputs": [],
   "source": [
    "print(rec)\n",
    "plt.hist(rec)\n",
    "plt.title(\"Recall for each image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1932ed81-9d0d-41c5-8305-b7015e088a79",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 469
    },
    "id": "1932ed81-9d0d-41c5-8305-b7015e088a79",
    "outputId": "f3234eea-45b3-485e-9e6a-20204b664ffd"
   },
   "outputs": [],
   "source": [
    "print(iou)\n",
    "plt.hist(iou)\n",
    "plt.title(\"Intersection over Union for each image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67274767-d36d-4c80-8459-0ef88b0bff36",
   "metadata": {
    "id": "67274767-d36d-4c80-8459-0ef88b0bff36"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "highway_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
